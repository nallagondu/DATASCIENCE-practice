{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1cdf0g7u_inwL5yUnpCNFcfnO-GYhu2R4",
      "authorship_tag": "ABX9TyPNWtA1OFDkxTLvyauvuZb6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nallagondu/DATASCIENCE-practice/blob/featurebranch/GICC_INC_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vyz3ipZITS6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from google.colab import drive\n",
        "from matplotlib import pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LyyozugfW-9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from Google Drive\n",
        "data_path = '/content/drive/MyDrive/Colab Notebooks/DATA for Colab/incident52000.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "gwV4ceG7XQ5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()\n"
      ],
      "metadata": {
        "id": "pCnilainhX0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mEYE9FXHfGQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape\n"
      ],
      "metadata": {
        "id": "hQIv78eZignz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "EmlPkU51CrY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values = df.isnull().sum()\n",
        "missing_values"
      ],
      "metadata": {
        "id": "acIkJJnNC1tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates()"
      ],
      "metadata": {
        "id": "Qb8VgV2YDHdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "A4fL--k6DQzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['Activity due','Work start','Timeline','Additional assignee list', 'Approval set', 'Due Date', 'SLA due'], inplace=True)"
      ],
      "metadata": {
        "id": "OvjTqMPUDkwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "I0DOEy7JD70c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mode_assigned_to = df['Assigned to'].mode()[0]  # Calculate mode (most frequent value)\n",
        "df['Assigned to'].fillna(mode_assigned_to, inplace=True)"
      ],
      "metadata": {
        "id": "TuHgFEJeEALR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "TIHE0iF2EG8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract server name\n",
        "def extract_server_name(description):\n",
        "    parts = description.split('_-_')\n",
        "    server_name = parts[-1].strip()\n",
        "    server_name = server_name.replace('[Server]', '').strip()\n",
        "    return server_name\n",
        "\n",
        "# Apply the function to extract server names only for rows where 'Causal Configuration Item' is empty\n",
        "mask = df['Causal Configuration Item'].isnull()\n",
        "df.loc[mask, 'Causal Configuration Item'] = df.loc[mask, 'Short Description'].apply(extract_server_name)\n",
        "\n",
        "# Print the DataFrame with the extracted server names filled into the 'Causal Configuration Item' column\n",
        "print(df)"
      ],
      "metadata": {
        "id": "CAkawVBOGiEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "ovhg4mQbGpvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "pTCa_f69HFv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill empty values in 'Resolved' with corresponding values from 'Created'\n",
        "df['Resolved'].fillna(df['Created'], inplace=True)\n",
        "df"
      ],
      "metadata": {
        "id": "FPKV_kUWHgbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "D5GiVYxbHmWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n"
      ],
      "metadata": {
        "id": "poiTMtlsgCiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "8-0jLmAhI-H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract server name from 'Short Description' column\n",
        "df['Server'] = df['Causal Configuration Item'].str.split('_-_').str[-1].str.strip()\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "df['Server'] = label_encoder.fit_transform(df['Server'])\n",
        "# Save the DataFrame to cleaned.csv\n",
        "df.to_csv('cleanedData.csv', index=False)\n",
        "df"
      ],
      "metadata": {
        "id": "qzptT8DZJFbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Created by'] = label_encoder.fit_transform(df['Created by'])\n",
        "df['Severity'] = label_encoder.fit_transform(df['Severity'])\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "4eMtMfZdlhXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values in categorical columns with the most frequent value\n",
        "categorical_cols = ['Caller', 'Causal Configuration Item', 'Description', 'Priority', 'Assigned to', 'State', 'Assignment group', 'Server']\n",
        "for col in categorical_cols:\n",
        "    df[col].fillna(df[col].mode()[0], inplace=True)"
      ],
      "metadata": {
        "id": "MTUaSnqph5Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values in numerical columns with median or mean\n",
        "numerical_cols = ['Business duration', 'Child Incidents']\n",
        "for col in numerical_cols:\n",
        "    df[col].fillna(df[col].median(), inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "uG3DpV2uiIKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert categorical variables into numerical representations using label encoding\n",
        "#label_encoder = LabelEncoder()\n",
        "#for col in categorical_cols:\n",
        "  #  df[col] = label_encoder.fit_transform(df[col])\n",
        "\n",
        "# Encode categorical columns: Caller, Server\n",
        "label_encoder = LabelEncoder()\n",
        "df['Caller'] = label_encoder.fit_transform(df['Caller'])\n",
        "df"
      ],
      "metadata": {
        "id": "x9nIZBXUiLIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "lgNCuvVyQcqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Check if the input is a list\n",
        "    if isinstance(text, list):\n",
        "        # Join list elements into a single string\n",
        "        text = ' '.join(text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    return text"
      ],
      "metadata": {
        "id": "FIRcIpGrRZQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply text preprocessing\n",
        "df['Short Description'] = df['Short Description'].apply(preprocess_text)\n",
        "df['Description'] = df['Description'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "_s6sXSasRvyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GcIHKuiVLhO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process Short Description column by splitting into chunks\n",
        "# Define a function to split text into chunks\n",
        "#def split_into_chunks(text, chunk_size=50):\n",
        " #   chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        " #   return chunks"
      ],
      "metadata": {
        "id": "nAu-aak-JInt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function to Short Description column\n",
        "#df['Short Description'] = df['Short Description'].apply(split_into_chunks)\n",
        "\n",
        "# Check the updated DataFrame\n",
        "#print(df.head())"
      ],
      "metadata": {
        "id": "QJnM7QzFJQS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform retrieval and count occurrences for a specific server or based on description\n",
        "def retrieve_and_count_occurrences(query, index, vectorizer, dataset):\n",
        "    query_vector = vectorizer.transform([preprocess_text(query)]).toarray()\n",
        "\n",
        "    # Calculate cosine similarity between query vector and vectors in the index\n",
        "    similarity_scores = {}\n",
        "    for i, vector in index.items():\n",
        "        similarity = cosine_similarity(query_vector, [vector])[0][0]\n",
        "        similarity_scores[i] = similarity\n",
        "\n",
        "    # Retrieve top similar documents for the specific server\n",
        "    top_documents = sorted(similarity_scores, key=similarity_scores.get, reverse=True)[:len(similarity_scores)]\n",
        "    top_documents_info = [(dataset.iloc[i]['Causal Configuration Item'], dataset.iloc[i]['Description']) for i in top_documents if query.lower() in dataset.iloc[i]['Causal Configuration Item'].lower()]\n",
        "\n",
        "    # If no logs found for the given server, analyze based on Description and Short Description\n",
        "    if not top_documents_info:\n",
        "        # Analyze based on Description and Short Description\n",
        "        similar_issues = {}\n",
        "        for desc in dataset['Description']:\n",
        "            if query.lower() in desc.lower():\n",
        "                if desc in similar_issues:\n",
        "                    similar_issues[desc] += 1\n",
        "                else:\n",
        "                    similar_issues[desc] = 1\n",
        "        for short_desc in dataset['Short Description']:\n",
        "            if query.lower() in short_desc.lower():\n",
        "                if short_desc in similar_issues:\n",
        "                    similar_issues[short_desc] += 1\n",
        "                else:\n",
        "                    similar_issues[short_desc] = 1\n",
        "\n",
        "        if not similar_issues:\n",
        "            return \"No logs or similar issues found for the given server or description.\", None\n",
        "\n",
        "        most_repeated_issue = max(similar_issues, key=similar_issues.get)\n",
        "        return \"No logs found for the given server. Analyzing based on Description and Short Description.\", most_repeated_issue\n",
        "\n",
        "    # Analyze repeated issues for the specific server\n",
        "    issues_count = {}\n",
        "    for _, desc in top_documents_info:\n",
        "        if query.lower() in desc.lower():\n",
        "            if desc in issues_count:\n",
        "                issues_count[desc] += 1\n",
        "            else:\n",
        "                issues_count[desc] = 1\n",
        "\n",
        "    # Find the most repeated issue for the specific server\n",
        "    most_repeated_issue = max(issues_count, key=issues_count.get)\n",
        "\n",
        "    return len(top_documents_info), most_repeated_issue\n",
        "\n",
        "# Example of retrieval and occurrences count for a specific server or based on description\n",
        "query = \"MESPWERIAT03\"  # Replace with your query\n",
        "\n",
        "result = retrieve_and_count_occurrences(query, short_description_index, tfidf_vectorizer, df)\n",
        "if isinstance(result, tuple):\n",
        "    occurrences_count, most_repeated_issue = result\n",
        "    print(f\"Occurrences count for server '{query}': {occurrences_count}\")\n",
        "    print(f\"Most repeated issue for server '{query}': {most_repeated_issue}\")\n",
        "else:\n",
        "    print(result)  # No logs or similar issues found message\n"
      ],
      "metadata": {
        "id": "PD9SrlDdJbx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform retrieval and count occurrences for a specific server or based on description\n",
        "def retrieve_and_count_occurrences(query, index, vectorizer, dataset):\n",
        "    query_vector = vectorizer.transform([preprocess_text(query)]).toarray()\n",
        "\n",
        "    # Calculate cosine similarity between query vector and vectors in the index\n",
        "    similarity_scores = {}\n",
        "    for i, vector in index.items():\n",
        "        similarity = cosine_similarity(query_vector, [vector])[0][0]\n",
        "        similarity_scores[i] = similarity\n",
        "\n",
        "    # Retrieve top similar issue for the specific server\n",
        "    top_documents = sorted(similarity_scores, key=similarity_scores.get, reverse=True)[:len(similarity_scores)]\n",
        "    top_documents_info = [(dataset.iloc[i]['Causal Configuration Item'], dataset.iloc[i]['Description']) for i in top_documents if query.lower() in dataset.iloc[i]['Causal Configuration Item'].lower()]\n",
        "\n",
        "    # If no logs found for the given server, analyze based on Description and Short Description\n",
        "    if not top_documents_info:\n",
        "        # Analyze based on Description and Short Description\n",
        "        similar_issues = {}\n",
        "        for desc in dataset['Description']:\n",
        "            if query.lower() in desc.lower():\n",
        "                if desc in similar_issues:\n",
        "                    similar_issues[desc] += 1\n",
        "                else:\n",
        "                    similar_issues[desc] = 1\n",
        "        for short_desc in dataset['Short Description']:\n",
        "            if query.lower() in short_desc.lower():\n",
        "                if short_desc in similar_issues:\n",
        "                    similar_issues[short_desc] += 1\n",
        "                else:\n",
        "                    similar_issues[short_desc] = 1\n",
        "\n",
        "        if not similar_issues:\n",
        "            return \"No logs or similar issues found for the given server or description.\", None\n",
        "\n",
        "        most_repeated_issue = max(similar_issues, key=similar_issues.get)\n",
        "        return \"No logs found for the given server. Analyzing based on Description and Short Description.\", similar_issues\n",
        "\n",
        "    # Analyze repeated issues for the specific server\n",
        "    issues_count = {}\n",
        "    for _, desc in top_documents_info:\n",
        "        if query.lower() in desc.lower():\n",
        "            if desc in issues_count:\n",
        "                issues_count[desc] += 1\n",
        "            else:\n",
        "                issues_count[desc] = 1\n",
        "\n",
        "    # Find the most repeated issue for the specific server\n",
        "    most_repeated_issue = max(issues_count, key=issues_count.get)\n",
        "\n",
        "    return len(top_documents_info), issues_count, most_repeated_issue\n",
        "\n",
        "# Example of retrieval and occurrences count for a specific server or based on description\n",
        "query = \"MESPWERIAT03\"  # Replace with your query\n",
        "\n",
        "result = retrieve_and_count_occurrences(query, short_description_index, tfidf_vectorizer, df)\n",
        "if isinstance(result, tuple):\n",
        "    occurrences_count, issues_count, most_repeated_issue = result\n",
        "    print(f\"Occurrences count for server '{query}': {occurrences_count}\")\n",
        "    print(f\"Most repeated issue for server '{query}': {most_repeated_issue}\")\n",
        "    print(\"Issues Count:\")\n",
        "    for issue, frequency in issues_count.items():\n",
        "        print(f\"Issue: {issue}, Frequency: {frequency}\")\n",
        "else:\n",
        "    print(result)  # No logs or similar issues found message\n"
      ],
      "metadata": {
        "id": "vP6bt1twwXqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5vvcC0bXiL0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example of retrieval and occurrences count for a specific server or based on description\n",
        "# Please enter the query details in query = \"<xxxxxx>\"  # Replace with your query\n"
      ],
      "metadata": {
        "id": "Ads1_ze4hQ_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Function to perform retrieval and count occurrences for a specific server or based on description\n",
        "def retrieve_and_count_occurrences(query, index, vectorizer, dataset):\n",
        "    query_vector = vectorizer.transform([preprocess_text(query)]).toarray()\n",
        "\n",
        "    # Calculate cosine similarity between query vector and vectors in the index\n",
        "    similarity_scores = {}\n",
        "    for i, vector in index.items():\n",
        "        similarity = cosine_similarity(query_vector, [vector])[0][0]\n",
        "        similarity_scores[i] = similarity\n",
        "\n",
        "    # Retrieve top similar issue for the specific server\n",
        "    top_documents = sorted(similarity_scores, key=similarity_scores.get, reverse=True)[:len(similarity_scores)]\n",
        "    top_documents_info = [(dataset.iloc[i]['Causal Configuration Item'], dataset.iloc[i]['Description'], dataset.iloc[i]['Created']) for i in top_documents if query.lower() in dataset.iloc[i]['Causal Configuration Item'].lower()]\n",
        "\n",
        "    # If no logs found for the given server, analyze based on Description and Short Description\n",
        "    if not top_documents_info:\n",
        "        # Analyze based on Description and Short Description\n",
        "        similar_issues = defaultdict(list)\n",
        "        for i, desc in enumerate(dataset['Description']):\n",
        "            if query.lower() in desc.lower():\n",
        "                similar_issues[desc].append((dataset.iloc[i]['Causal Configuration Item'], dataset.iloc[i]['Created']))\n",
        "        for i, short_desc in enumerate(dataset['Short Description']):\n",
        "            if query.lower() in short_desc.lower():\n",
        "                similar_issues[short_desc].append((dataset.iloc[i]['Causal Configuration Item'], dataset.iloc[i]['Created']))\n",
        "\n",
        "        if not similar_issues:\n",
        "            return \"No logs or similar issues found for the given server or description.\", None\n",
        "\n",
        "        most_repeated_issue = max(similar_issues, key=lambda x: len(similar_issues[x]))\n",
        "        return \"No logs found for the given server. Analyzing based on Description and Short Description.\", similar_issues[most_repeated_issue], most_repeated_issue\n",
        "\n",
        "    # Analyze repeated issues for the specific server\n",
        "    issues_count = {}\n",
        "    for _, desc, created in top_documents_info:\n",
        "        if query.lower() in desc.lower():\n",
        "            if desc in issues_count:\n",
        "                issues_count[desc].append(created)\n",
        "            else:\n",
        "                issues_count[desc] = [created]\n",
        "\n",
        "    # Find the most repeated issue for the specific server\n",
        "    most_repeated_issue = max(issues_count, key=lambda x: len(issues_count[x]))\n",
        "\n",
        "    return len(top_documents_info), issues_count[most_repeated_issue], most_repeated_issue\n",
        "\n",
        "# Example of retrieval and occurrences count for a specific server or based on description\n",
        "query = \"RDU1PWCULBOT21\"  # Replace with your query\n",
        "\n",
        "result = retrieve_and_count_occurrences(query, short_description_index, tfidf_vectorizer, df)\n",
        "if isinstance(result, tuple):\n",
        "    occurrences_count, most_repeated_issue, most_repeated_issue_desc = result\n",
        "    print(f\"Occurrences count for '{query}': {occurrences_count}\")\n",
        "    print(f\"Most repeated issue for '{query}': {most_repeated_issue}\")\n",
        "    print(\"Issues Count:\")\n",
        "    for issue, frequency in issues_count.items():\n",
        "        print(f\"Issue: {issue}, Frequency: {frequency}\")\n",
        "else:\n",
        "    print(result)  # No logs or similar issues found message\n"
      ],
      "metadata": {
        "id": "xwRsW_AyMefR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**method 4**\n",
        " ##  to find the data accurancy based on any word or server name and it count the number of accurancy"
      ],
      "metadata": {
        "id": "FdUIlfRrg2uk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #is to find the data accurancy based on any word or server name and it count the number of accurancy\n",
        "def find_word_in_dataframe(word, dataframe):\n",
        "    occurrences_count = 0\n",
        "    matching_rows = []\n",
        "\n",
        "    # Iterate through each row in the DataFrame\n",
        "    for index, row in dataframe.iterrows():\n",
        "        # Check if the word is present in any of the relevant columns\n",
        "        if (word.lower() in str(row['Causal Configuration Item']).lower() or\n",
        "                word.lower() in str(row['Description']).lower() or\n",
        "                word.lower() in str(row['Short Description']).lower() or\n",
        "                word.lower() in str(row['Created']).lower()):  # Assuming 'Created' is the column name for date\n",
        "            occurrences_count += 1\n",
        "            matching_rows.append((row['Causal Configuration Item'], row['Description'], row['Short Description'], row['Created']))\n",
        "\n",
        "    return occurrences_count, matching_rows\n",
        "\n",
        "# Example usage\n",
        "word = \"access \"  # Replace with the word you want to search for\n",
        "occurrences_count, matching_rows = find_word_in_dataframe(word, df)\n",
        "\n",
        "print(f\"Occurrences count for '{word}': {occurrences_count}\")\n",
        "if matching_rows:\n",
        "    print(\"Matching Rows:\")\n",
        "    for row in matching_rows:\n",
        "        print(row)\n",
        "else:\n",
        "    print(\"No matches found.\")\n"
      ],
      "metadata": {
        "id": "u2y_xnAiYAxO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}