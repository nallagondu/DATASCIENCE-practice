{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1cdf0g7u_inwL5yUnpCNFcfnO-GYhu2R4",
      "authorship_tag": "ABX9TyP6DHknlLj2XguqkS0hNbTu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nallagondu/DATASCIENCE-practice/blob/main/GICC_INC_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vyz3ipZITS6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from google.colab import drive\n",
        "from matplotlib import pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LyyozugfW-9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from Google Drive\n",
        "data_path = '/content/drive/MyDrive/Colab Notebooks/DATA for Colab/incident52000.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "gwV4ceG7XQ5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()\n"
      ],
      "metadata": {
        "id": "pCnilainhX0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mEYE9FXHfGQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape\n"
      ],
      "metadata": {
        "id": "hQIv78eZignz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "EmlPkU51CrY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values = df.isnull().sum()\n",
        "missing_values"
      ],
      "metadata": {
        "id": "acIkJJnNC1tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates()"
      ],
      "metadata": {
        "id": "Qb8VgV2YDHdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "A4fL--k6DQzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['Activity due','Work start','Timeline','Additional assignee list', 'Approval set', 'Due Date', 'SLA due'], inplace=True)"
      ],
      "metadata": {
        "id": "OvjTqMPUDkwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "I0DOEy7JD70c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mode_assigned_to = df['Assigned to'].mode()[0]  # Calculate mode (most frequent value)\n",
        "df['Assigned to'].fillna(mode_assigned_to, inplace=True)"
      ],
      "metadata": {
        "id": "TuHgFEJeEALR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "TIHE0iF2EG8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract server name\n",
        "def extract_server_name(description):\n",
        "    parts = description.split('_-_')\n",
        "    server_name = parts[-1].strip()\n",
        "    server_name = server_name.replace('[Server]', '').strip()\n",
        "    return server_name\n",
        "\n",
        "# Apply the function to extract server names only for rows where 'Causal Configuration Item' is empty\n",
        "mask = df['Causal Configuration Item'].isnull()\n",
        "df.loc[mask, 'Causal Configuration Item'] = df.loc[mask, 'Short Description'].apply(extract_server_name)\n",
        "\n",
        "# Print the DataFrame with the extracted server names filled into the 'Causal Configuration Item' column\n",
        "print(df)"
      ],
      "metadata": {
        "id": "CAkawVBOGiEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "ovhg4mQbGpvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "pTCa_f69HFv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill empty values in 'Resolved' with corresponding values from 'Created'\n",
        "df['Resolved'].fillna(df['Created'], inplace=True)\n",
        "df"
      ],
      "metadata": {
        "id": "FPKV_kUWHgbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "D5GiVYxbHmWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n"
      ],
      "metadata": {
        "id": "poiTMtlsgCiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "8-0jLmAhI-H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract server name from 'Short Description' column\n",
        "df['Server'] = df['Causal Configuration Item'].str.split('_-_').str[-1].str.strip()\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "df['Server'] = label_encoder.fit_transform(df['Server'])\n",
        "# Save the DataFrame to cleaned.csv\n",
        "df.to_csv('cleanedData.csv', index=False)\n",
        "df"
      ],
      "metadata": {
        "id": "qzptT8DZJFbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Created by'] = label_encoder.fit_transform(df['Created by'])\n",
        "df['Severity'] = label_encoder.fit_transform(df['Severity'])\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "4eMtMfZdlhXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values in categorical columns with the most frequent value\n",
        "categorical_cols = ['Caller', 'Causal Configuration Item', 'Description', 'Priority', 'Assigned to', 'State', 'Assignment group', 'Server']\n",
        "for col in categorical_cols:\n",
        "    df[col].fillna(df[col].mode()[0], inplace=True)"
      ],
      "metadata": {
        "id": "MTUaSnqph5Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values in numerical columns with median or mean\n",
        "numerical_cols = ['Business duration', 'Child Incidents']\n",
        "for col in numerical_cols:\n",
        "    df[col].fillna(df[col].median(), inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "uG3DpV2uiIKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert categorical variables into numerical representations using label encoding\n",
        "#label_encoder = LabelEncoder()\n",
        "#for col in categorical_cols:\n",
        "  #  df[col] = label_encoder.fit_transform(df[col])\n",
        "\n",
        "# Encode categorical columns: Caller, Server\n",
        "label_encoder = LabelEncoder()\n",
        "df['Caller'] = label_encoder.fit_transform(df['Caller'])\n",
        "df"
      ],
      "metadata": {
        "id": "x9nIZBXUiLIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "lgNCuvVyQcqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Check if the input is a list\n",
        "    if isinstance(text, list):\n",
        "        # Join list elements into a single string\n",
        "        text = ' '.join(text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Implement any additional preprocessing steps here\n",
        "    return text"
      ],
      "metadata": {
        "id": "FIRcIpGrRZQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply text preprocessing\n",
        "df['Short Description'] = df['Short Description'].apply(preprocess_text)\n",
        "df['Description'] = df['Description'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "_s6sXSasRvyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorization using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n",
        "X_short_description = tfidf_vectorizer.fit_transform(df['Short Description']).toarray()\n",
        "X_description = tfidf_vectorizer.fit_transform(df['Description']).toarray()\n",
        "\n",
        "# Indexing\n",
        "short_description_index = {i: vector for i, vector in enumerate(X_short_description)}\n",
        "description_index = {i: vector for i, vector in enumerate(X_description)}\n",
        "\n",
        "# Function to perform retrieval\n",
        "def retrieve_similar_documents(query, index, vectorizer, dataset):\n",
        "    query_vector = vectorizer.transform([preprocess_text(query)]).toarray()\n",
        "\n",
        "    # Calculate cosine similarity between query vector and vectors in the index\n",
        "    similarity_scores = {}\n",
        "    for i, vector in index.items():\n",
        "        similarity = cosine_similarity(query_vector, [vector])[0][0]\n",
        "        similarity_scores[i] = similarity\n",
        "\n",
        "    # Retrieve top similar documents\n",
        "    top_k = 10  # Number of top similar documents to retrieve\n",
        "    top_documents = sorted(similarity_scores, key=similarity_scores.get, reverse=True)[:top_k]\n",
        "    top_documents_info = [(dataset.iloc[i]['Short Description'], dataset.iloc[i]['Description']) for i in top_documents]\n",
        "\n",
        "    return top_documents_info\n",
        "\n",
        "# Example of retrieval\n",
        "query = \"Ping Down\"  # Replace with your query\n",
        "\n",
        "print(\"Top similar documents based on Short Description:\")\n",
        "top_short_description = retrieve_similar_documents(query, short_description_index, tfidf_vectorizer, df)\n",
        "for short_desc, desc in top_short_description:\n",
        "    print(short_desc)\n",
        "\n",
        "print(\"\\nTop similar documents based on Description:\")\n",
        "top_description = retrieve_similar_documents(query, description_index, tfidf_vectorizer, df)\n",
        "for short_desc, desc in top_description:\n",
        "    print(desc)"
      ],
      "metadata": {
        "id": "tSOhSiXpRz0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform retrieval and count occurrences\n",
        "def retrieve_and_count_occurrences(query, index, vectorizer, dataset):\n",
        "    query_vector = vectorizer.transform([preprocess_text(query)]).toarray()\n",
        "\n",
        "    # Calculate cosine similarity between query vector and vectors in the index\n",
        "    similarity_scores = {}\n",
        "    for i, vector in index.items():\n",
        "        similarity = cosine_similarity(query_vector, [vector])[0][0]\n",
        "        similarity_scores[i] = similarity\n",
        "\n",
        "    # Retrieve top similar documents\n",
        "    top_k = 10  # Number of top similar documents to retrieve\n",
        "    top_documents = sorted(similarity_scores, key=similarity_scores.get, reverse=True)[:top_k]\n",
        "    top_documents_info = [(dataset.iloc[i]['Short Description'], dataset.iloc[i]['Description']) for i in top_documents]\n",
        "\n",
        "    # Count occurrences of query in the dataset\n",
        "    occurrences_count = sum(1 for _, desc in top_documents_info if query.lower() in desc.lower())\n",
        "\n",
        "    return occurrences_count, top_documents_info\n",
        "\n",
        "# Example of retrieval and occurrences count\n",
        "query = \"AUSPWESUPCSDB01\"  # Replace with your query\n",
        "\n",
        "occurrences_count_short_description, top_short_description = retrieve_and_count_occurrences(query, short_description_index, tfidf_vectorizer, df)\n",
        "print(f\"Occurrences count based on Short Description: {occurrences_count_short_description}\")\n",
        "for short_desc, desc in top_short_description:\n",
        "    print(short_desc)\n",
        "\n",
        "occurrences_count_description, top_description = retrieve_and_count_occurrences(query, description_index, tfidf_vectorizer, df)\n",
        "print(f\"\\nOccurrences count based on Description: {occurrences_count_description}\")\n",
        "for short_desc, desc in top_description:\n",
        "    print(desc)\n"
      ],
      "metadata": {
        "id": "GcIHKuiVLhO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform retrieval and count occurrences\n",
        "def retrieve_and_count_occurrences(query, index, vectorizer, dataset):\n",
        "    query_vector = vectorizer.transform([preprocess_text(query)]).toarray()\n",
        "\n",
        "    # Calculate cosine similarity between query vector and vectors in the index\n",
        "    similarity_scores = {}\n",
        "    for i, vector in index.items():\n",
        "        similarity = cosine_similarity(query_vector, [vector])[0][0]\n",
        "        similarity_scores[i] = similarity\n",
        "\n",
        "    # Retrieve top similar documents\n",
        "    top_k = 10  # Number of top similar documents to retrieve\n",
        "    top_documents = sorted(similarity_scores, key=similarity_scores.get, reverse=True)[:top_k]\n",
        "    top_documents_info = [(dataset.iloc[i]['Causal Configuration Item'], dataset.iloc[i]['Description']) for i in top_documents]\n",
        "\n",
        "    # Count occurrences of query in the dataset\n",
        "    occurrences_count = sum(1 for server, _ in top_documents_info if query.lower() in server.lower())\n",
        "\n",
        "    # Analyze repeated issues\n",
        "    issues_count = {}\n",
        "    for _, desc in top_documents_info:\n",
        "        if query.lower() in desc.lower():\n",
        "            if desc in issues_count:\n",
        "                issues_count[desc] += 1\n",
        "            else:\n",
        "                issues_count[desc] = 1\n",
        "\n",
        "    # Find the most repeated issue\n",
        "    most_repeated_issue = max(issues_count, key=issues_count.get)\n",
        "\n",
        "    return occurrences_count, most_repeated_issue, top_documents_info\n",
        "\n",
        "# Example of retrieval and occurrences count\n",
        "query = \"AUSPWESUPCSDB01\"  # Replace with your query\n",
        "\n",
        "occurrences_count_short_description, most_repeated_issue_short_description, top_short_description = retrieve_and_count_occurrences(query, short_description_index, tfidf_vectorizer, df)\n",
        "print(f\"Occurrences count based on Short Description: {occurrences_count_short_description}\")\n",
        "print(f\"Most repeated issue based on Short Description: {most_repeated_issue_short_description}\")\n",
        "for server, desc in top_short_description:\n",
        "    print(f\"Server: {server}, Issue: {desc}\")\n",
        "\n",
        "occurrences_count_description, most_repeated_issue_description, top_description = retrieve_and_count_occurrences(query, description_index, tfidf_vectorizer, df)\n",
        "print(f\"\\nOccurrences count based on Description: {occurrences_count_description}\")\n",
        "print(f\"Most repeated issue based on Description: {most_repeated_issue_description}\")\n",
        "for server, desc in top_description:\n",
        "    print(f\"Server: {server}, Issue: {desc}\")\n"
      ],
      "metadata": {
        "id": "HOh1igaMY3_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process Short Description column by splitting into chunks\n",
        "# Define a function to split text into chunks\n",
        "#def split_into_chunks(text, chunk_size=50):\n",
        " #   chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        " #   return chunks"
      ],
      "metadata": {
        "id": "nAu-aak-JInt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function to Short Description column\n",
        "#df['Short Description'] = df['Short Description'].apply(split_into_chunks)\n",
        "\n",
        "# Check the updated DataFrame\n",
        "#print(df.head())"
      ],
      "metadata": {
        "id": "QJnM7QzFJQS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "PD9SrlDdJbx6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}